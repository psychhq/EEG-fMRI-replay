{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f55d00d-4e59-40a0-ad35-b4709a8cbdc8",
   "metadata": {
    "tags": []
   },
   "source": [
    "SCRIPT INFORMATION:\n",
    "SCRIPT: CREATE BINARIZED MASKS FROM SEGMENTED FUNCTIONAL IMAGES\n",
    "PROJECT: FMRIREPLAY\n",
    "WRITTEN BY QI HUANG, 2022\n",
    "CONTACT: STATE KEY LABORATORY OF COGNITVIE NERUOSCIENCE AND LEARNING, BEIJING NORMAL UNIVERSITY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3792da7b-d14f-4f2a-a3ec-640f11dfa627",
   "metadata": {},
   "source": [
    "# ======================================================================\n",
    "# IMPORT RELEVANT PACKAGES\n",
    "# ======================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "61cccb03-956a-48c7-8654-2833887b3329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basic libraries:\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import warnings\n",
    "from os.path import join as opj\n",
    "# import nipype libraries:\n",
    "from nipype.interfaces.utility import IdentityInterface\n",
    "from nipype.interfaces.io import SelectFiles, DataSink\n",
    "from nipype.pipeline.engine import Workflow, Node, MapNode\n",
    "# import libraries for bids interaction:\n",
    "from bids.layout import BIDSLayout\n",
    "# import freesurfer interfaces:\n",
    "from nipype.interfaces.freesurfer import Binarize\n",
    "# import fsl interfaces:\n",
    "from niflow.nipype1.workflows.fmri.fsl import create_susan_smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db15e89-c215-43cb-97b5-0c3688a31f25",
   "metadata": {},
   "source": [
    "# ======================================================================\n",
    "# ENVIRONMENT SETTINGS (DEALING WITH ERRORS AND WARNINGS):\n",
    "# ======================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9bc1550e-11b3-42a1-b1e0-4dd8fc38ef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the fsl output type environment variable:\n",
    "os.environ['FSLOUTPUTTYPE'] = 'NIFTI_GZ'\n",
    "# set the freesurfer subject directory:\n",
    "os.environ['SUBJECTS_DIR'] = '/home/user01/subjects'\n",
    "# deal with nipype-related warnings:\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\"\n",
    "# filter out warnings related to the numpy package:\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19b1b9c-9bce-42e2-8236-e937d972e532",
   "metadata": {},
   "source": [
    "# ======================================================================\n",
    "# DEFINE PBS CLUSTER JOB TEMPLATE (NEEDED WHEN RUNNING ON THE CLUSTER):\n",
    "# ======================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ddf1c4f2-7fd3-48c6-855a-9c493b5a9b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_template = \"\"\"\n",
    "#PBS -l walltime=5:00:00\n",
    "#PBS -j oe\n",
    "#PBS -o $HOME/fMRIreplay_hq/fmrireplay-masks/logs\n",
    "#PBS -m n\n",
    "#PBS -v FSLOUTPUTTYPE=NIFTI_GZ  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb023cc-6a8a-42a5-88c8-227acbeb434f",
   "metadata": {},
   "source": [
    "# ======================================================================\n",
    "# DEFINE PATHS AND SUBJECTS\n",
    "# ======================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0260b5db-8466-4b84-a273-15bc5cb53064",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = None\n",
    "sub_list = None\n",
    "# path to the project root:\n",
    "project_name = 'fMRIreplay_hq'\n",
    "path_root = opj(os.getcwd().split(project_name)[0] ,'fMRIreplay_hq')\n",
    "# grab the list of subjects from the bids data set:\n",
    "path_bids = opj(path_root, 'fmrireplay-bids','BIDS')\n",
    "path_fmriprep = opj(path_root,'fmrireplay-fmriprep')\n",
    "path_masks = opj(path_root,'fmrireplay-masks')\n",
    "# dl.get(opj('bids', 'participants.json'))\n",
    "# dl.get(glob.glob(opj(path_root, 'BIDSdata', 'sub-*', '*', '*.json')))\n",
    "# dl.get(glob.glob(opj(path_root, 'BIDSdata', 'sub-*', '*.json')))\n",
    "# dl.get(glob.glob(opj(path_root, 'BIDSdata', '*.json')))\n",
    "layout = BIDSLayout(path_bids,derivatives=True)\n",
    "# get all subject ids:\n",
    "sub_list = sorted(layout.get_subjects())\n",
    "# create a template to add the \"sub-\" prefix to the ids\n",
    "sub_template = ['sub-'] * len(sub_list)\n",
    "# add the prefix to all ids:\n",
    "sub_list = [\"%s%s\" % t for t in zip(sub_template, sub_list)]\n",
    "# if user defined to run specific subject\n",
    "# sub_list = sub_list[int(sys.argv[1]):int(sys.argv[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a21b29ca-972d-49ac-aecc-aebd71e2d7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_list = ['sub-05']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e684c3-1e92-4663-bcc3-d13ea91c4a8a",
   "metadata": {},
   "source": [
    "# ======================================================================\n",
    "# DEFINE NODE: INFOSOURCE\n",
    "# ======================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a19ccc28-c6e3-4d83-8033-1a80dd050545",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the infosource node that collects the data:\n",
    "infosource = Node(IdentityInterface(\n",
    "    fields=['subject_id']), name='infosource')\n",
    "# let the node iterate (parallelize) over all subjects:\n",
    "infosource.iterables = [('subject_id', sub_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702e4648-f6ce-4f14-bd56-f4b1ee0d0825",
   "metadata": {},
   "source": [
    "# ======================================================================\n",
    "# DEFINE SELECTFILES NODE\n",
    "# ======================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "00e759c7-20be-467e-9ca0-eb5be41a5291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl.get(glob.glob(path_func))\n",
    "# dl.get(glob.glob(path_func_parc))\n",
    "# dl.get(glob.glob(path_wholemask))\n",
    "# define all relevant files paths:\n",
    "# templates = dict(\n",
    "#     func=opj(path_fmriprep, '{subject_id}', \n",
    "#              'func', '*space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz'),\n",
    "#     func_parc=opj(path_fmriprep,  '{subject_id}',\n",
    "#              'func', '*space-MNI152NLin2009cAsym_desc-aparcaseg_dseg.nii.gz'),\n",
    "#     wholemask=opj(path_fmriprep, '{subject_id}',\n",
    "#          'func', '*space-MNI152NLin2009cAsym_desc-brain_mask.nii.gz'),\n",
    "# )\n",
    "templates = dict(\n",
    "    func=opj(path_fmriprep, '{subject_id}', \n",
    "             'func', '*space-T1w*preproc_bold.nii.gz'),\n",
    "    func_parc=opj(path_fmriprep,  '{subject_id}',\n",
    "             'func', '*space-T1w*aparcaseg_dseg.nii.gz'),\n",
    "    wholemask=opj(path_fmriprep, '{subject_id}',\n",
    "         'func', '*space-T1w*brain_mask.nii.gz'),\n",
    ")\n",
    "# define the selectfiles node:\n",
    "selectfiles = Node(SelectFiles(templates, sort_filelist=True),\n",
    "                   name='selectfiles')\n",
    "# set expected thread and memory usage for the node:\n",
    "selectfiles.interface.num_threads = 1\n",
    "selectfiles.interface.estimated_memory_gb = 0.1\n",
    "# selectfiles.inputs.subject_id = 'sub-004'\n",
    "# selectfiles_results = selectfiles.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb94320-3eeb-4c4b-baf5-704412f1275b",
   "metadata": {},
   "source": [
    "# ======================================================================\n",
    "# DEFINE CREATE_SUSAN_SMOOTH WORKFLOW NODE\n",
    "# ======================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b836a615-f87e-4e57-bb35-00c8dfbec7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the susan smoothing node and specify the smoothing fwhm:\n",
    "susan = create_susan_smooth()\n",
    "# set the smoothing kernel:\n",
    "susan.inputs.inputnode.fwhm = 4\n",
    "# set expected thread and memory usage for the nodes:\n",
    "susan.get_node('inputnode').interface.num_threads = 1\n",
    "susan.get_node('inputnode').interface.estimated_memory_gb = 0.1\n",
    "susan.get_node('median').interface.num_threads = 1\n",
    "susan.get_node('median').interface.estimated_memory_gb = 3\n",
    "susan.get_node('mask').interface.num_threads = 1\n",
    "susan.get_node('mask').interface.estimated_memory_gb = 3\n",
    "susan.get_node('meanfunc2').interface.num_threads = 1\n",
    "susan.get_node('meanfunc2').interface.estimated_memory_gb = 3\n",
    "susan.get_node('merge').interface.num_threads = 1\n",
    "susan.get_node('merge').interface.estimated_memory_gb = 3\n",
    "susan.get_node('multi_inputs').interface.num_threads = 1\n",
    "susan.get_node('multi_inputs').interface.estimated_memory_gb = 3\n",
    "susan.get_node('smooth').interface.num_threads = 1\n",
    "susan.get_node('smooth').interface.estimated_memory_gb = 3\n",
    "susan.get_node('outputnode').interface.num_threads = 1\n",
    "susan.get_node('outputnode').interface.estimated_memory_gb = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934641d5-d6a7-4880-8380-fe46a7feb6be",
   "metadata": {},
   "source": [
    "# ======================================================================\n",
    "# DEFINE BINARIZE NODE\n",
    "# ======================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ae52764b-23ed-4857-803e-0a30039628aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_visual_labels = [\n",
    "    1005, 2005,  # cuneus\n",
    "    1011, 2011,  # lateral occipital\n",
    "    1021, 2021,  # pericalcarine\n",
    "    1029, 2029,  # superioparietal\n",
    "    1013, 2013,  # lingual\n",
    "    1008, 2008,  # inferioparietal\n",
    "    1007, 2007,  # fusiform\n",
    "    1009, 2009,  # inferiotemporal\n",
    "    1016, 2016,  # parahippocampal\n",
    "    1015, 2015,  # middle temporal\n",
    "]\n",
    "mask_hippocampus_labels = [\n",
    "    17, 53,  # left and right hippocampus\n",
    "]\n",
    "mask_mtl_labels = [\n",
    "    17, 53,  # left and right hippocampus\n",
    "    1016, 2016,  # parahippocampal\n",
    "    1006, 2006,  # ctx-entorhinal\n",
    "]\n",
    "# function: use freesurfer mri_binarize to threshold an input volume\n",
    "mask_visual = MapNode(\n",
    "        interface=Binarize(), name='mask_visual',iterfield=['in_file'])\n",
    "# input: match instead of threshold, it\n",
    "mask_visual.inputs.match = mask_visual_labels\n",
    "# optimize the efficiency of the node:\n",
    "mask_visual.plugin_args = {'qsub_args': '-l nodes=1:ppn=1', 'overwrite': True}\n",
    "mask_visual.plugin_args = {'qsub_args': '-l mem=100MB', 'overwrite': True}\n",
    "\n",
    "# function: use freesurfer mri_binarize to threshold an input volume\n",
    "mask_hippocampus = MapNode(\n",
    "        interface=Binarize(), name='mask_hippocampus', iterfield=['in_file'])\n",
    "# input: match instead of threshold\n",
    "mask_hippocampus.inputs.match = mask_hippocampus_labels\n",
    "# optimize the efficiency of the node:\n",
    "mask_hippocampus.plugin_args = {\n",
    "    'qsub_args': '-l nodes=1:ppn=1', 'overwrite': True}\n",
    "mask_hippocampus.plugin_args = {\n",
    "    'qsub_args': '-l mem=100MB', 'overwrite': True}\n",
    "\n",
    "# function: use freesurfer mri_binarize to threshold an input volume\n",
    "mask_mtl = MapNode(\n",
    "        interface=Binarize(), name='mask_mtl', iterfield=['in_file'])\n",
    "# input: match instead of threshold\n",
    "mask_mtl.inputs.match = mask_mtl_labels\n",
    "# optimize the efficiency of the node:\n",
    "mask_mtl.plugin_args = {'qsub_args': '-l nodes=1:ppn=1', 'overwrite': True}\n",
    "mask_mtl.plugin_args = {'qsub_args': '-l mem=100MB', 'overwrite': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76ad04e-0b60-40be-8e8d-d5c48c65682a",
   "metadata": {},
   "source": [
    "# ======================================================================\n",
    "# CREATE DATASINK NODE\n",
    "# ======================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46f0174-b6a2-4711-9b0c-9168e8a2a383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a node of the function:\n",
    "datasink = Node(DataSink(), name='datasink')\n",
    "# assign the path to the base directory:\n",
    "datasink.inputs.base_directory = path_masks\n",
    "# create a list of substitutions to adjust the filepaths of datasink:\n",
    "substitutions = [('_subject_id_', '')]\n",
    "# assign the substitutions to the datasink command:\n",
    "datasink.inputs.substitutions = substitutions\n",
    "# determine whether to store output in parameterized form:\n",
    "datasink.inputs.parameterization = True\n",
    "# set expected thread and memory usage for the node:\n",
    "datasink.interface.num_threads = 1\n",
    "datasink.interface.estimated_memory_gb = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aeccb6-750c-489e-a67c-a5230ba68bdc",
   "metadata": {},
   "source": [
    "# ======================================================================\n",
    "# DEFINE WORKFLOW PIPELINE:\n",
    "# ======================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca03491d-ed34-49f7-b1d5-50e85bcc73b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiation of the 1st-level analysis workflow:\n",
    "wf = Workflow(name='masks')\n",
    "# stop execution of the workflow if an error is encountered:\n",
    "wf.config = {'execution': {'stop_on_first_crash': True}}\n",
    "# define the base directory of the workflow:\n",
    "wf.base_dir = opj(path_masks, 'work')\n",
    "# connect infosource to selectfiles node:\n",
    "wf.connect(infosource, 'subject_id', selectfiles, 'subject_id')\n",
    "# connect functional files to smoothing workflow:\n",
    "wf.connect(selectfiles, 'func', susan, 'inputnode.in_files')\n",
    "wf.connect(selectfiles, 'wholemask', susan, 'inputnode.mask_file')\n",
    "wf.connect(susan, 'outputnode.smoothed_files', datasink, 'smooth')\n",
    "# connect segmented functional files to visual mask node\n",
    "wf.connect(selectfiles, 'func_parc', mask_visual, 'in_file')\n",
    "wf.connect(mask_visual, 'binary_file', datasink, 'mask_visual.@binary')\n",
    "wf.connect(mask_visual, 'count_file', datasink, 'mask_visual.@count')\n",
    "# connect segmented functional files to hippocampus node\n",
    "wf.connect(selectfiles, 'func_parc', mask_hippocampus, 'in_file')\n",
    "wf.connect(mask_hippocampus, 'binary_file', datasink, 'mask_hippocampus.@binary')\n",
    "wf.connect(mask_hippocampus, 'count_file', datasink, 'mask_hippocampus.@count')\n",
    "# connect segmented functional files to mtl node\n",
    "wf.connect(selectfiles, 'func_parc', mask_mtl, 'in_file')\n",
    "wf.connect(mask_mtl, 'binary_file', datasink, 'mask_mtl.@binary')\n",
    "wf.connect(mask_mtl, 'count_file', datasink, 'mask_mtl.@count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b64cc2-bc96-4fd9-82db-ed733554d7f1",
   "metadata": {},
   "source": [
    "# ======================================================================\n",
    "# WRITE GRAPH AND EXECUTE THE WORKFLOW\n",
    "# ======================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7dc584eb-c4c7-4310-aef4-b73b9be7d375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220621-21:40:19,942 nipype.workflow INFO:\n",
      "\t Generated workflow graph: /home/user01/fMRIreplay_hq/fmrireplay-masks/work/masks/graph.png (graph2use=colored, simple_form=True).\n",
      "220621-21:40:19,947 nipype.workflow INFO:\n",
      "\t Workflow masks settings: ['check', 'execution', 'logging', 'monitoring']\n",
      "220621-21:40:19,953 nipype.workflow INFO:\n",
      "\t Running serially.\n",
      "220621-21:40:19,954 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"masks.selectfiles\" in \"/home/user01/fMRIreplay_hq/fmrireplay-masks/work/masks/_subject_id_sub-05/selectfiles\".\n",
      "220621-21:40:19,957 nipype.workflow INFO:\n",
      "\t [Node] Executing \"selectfiles\" <nipype.interfaces.io.SelectFiles>\n",
      "220621-21:40:19,960 nipype.workflow INFO:\n",
      "\t [Node] Finished \"selectfiles\", elapsed time 0.002069s.\n",
      "220621-21:40:19,960 nipype.workflow WARNING:\n",
      "\t [Node] Error on \"masks.selectfiles\" (/home/user01/fMRIreplay_hq/fmrireplay-masks/work/masks/_subject_id_sub-05/selectfiles)\n",
      "220621-21:40:19,962 nipype.workflow ERROR:\n",
      "\t Node selectfiles.a0 failed to run on host liulabPriv.\n",
      "220621-21:40:19,962 nipype.workflow ERROR:\n",
      "\t Saving crash info to /home/user01/fMRIreplay_hq/fmrireplay-bids/BIDS/code/GLM/crash-20220621-214019-user01-selectfiles.a0-f8b9b1ef-3b5a-4cd8-b86a-b1bfa26d0c87.pklz\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/liulab/anaconda3/envs/huangqi_fmriprep/lib/python3.8/site-packages/nipype/pipeline/plugins/linear.py\", line 47, in run\n",
      "    node.run(updatehash=updatehash)\n",
      "  File \"/home/liulab/anaconda3/envs/huangqi_fmriprep/lib/python3.8/site-packages/nipype/pipeline/engine/nodes.py\", line 524, in run\n",
      "    result = self._run_interface(execute=True)\n",
      "  File \"/home/liulab/anaconda3/envs/huangqi_fmriprep/lib/python3.8/site-packages/nipype/pipeline/engine/nodes.py\", line 642, in _run_interface\n",
      "    return self._run_command(execute)\n",
      "  File \"/home/liulab/anaconda3/envs/huangqi_fmriprep/lib/python3.8/site-packages/nipype/pipeline/engine/nodes.py\", line 732, in _run_command\n",
      "    result.outputs = clean_working_directory(\n",
      "  File \"/home/liulab/anaconda3/envs/huangqi_fmriprep/lib/python3.8/site-packages/nipype/pipeline/engine/utils.py\", line 1469, in clean_working_directory\n",
      "    output_files.extend(walk_outputs(outputdict[output]))\n",
      "KeyError: 'subject_id'\n",
      "\n",
      "\n",
      "When creating this crashfile, the results file corresponding\n",
      "to the node could not be found.\n",
      "220621-21:40:19,964 nipype.workflow INFO:\n",
      "\t ***********************************\n",
      "220621-21:40:19,964 nipype.workflow ERROR:\n",
      "\t could not run node: masks.selectfiles.a0\n",
      "220621-21:40:19,964 nipype.workflow INFO:\n",
      "\t crashfile: /home/user01/fMRIreplay_hq/fmrireplay-bids/BIDS/code/GLM/crash-20220621-214019-user01-selectfiles.a0-f8b9b1ef-3b5a-4cd8-b86a-b1bfa26d0c87.pklz\n",
      "220621-21:40:19,965 nipype.workflow INFO:\n",
      "\t ***********************************\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'subject_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [194]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# write the graph:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m wf\u001b[38;5;241m.\u001b[39mwrite_graph(graph2use\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolored\u001b[39m\u001b[38;5;124m'\u001b[39m, simple_form\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mwf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/liulab/anaconda3/envs/huangqi_fmriprep/lib/python3.8/site-packages/nipype/pipeline/engine/workflows.py:638\u001b[0m, in \u001b[0;36mWorkflow.run\u001b[0;34m(self, plugin, plugin_args, updatehash)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m str2bool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexecution\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_report\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_report_info(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, execgraph)\n\u001b[0;32m--> 638\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdatehash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdatehash\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m datestr \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mutcnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m str2bool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexecution\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite_provenance\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n",
      "File \u001b[0;32m/home/liulab/anaconda3/envs/huangqi_fmriprep/lib/python3.8/site-packages/nipype/pipeline/plugins/linear.py:82\u001b[0m, in \u001b[0;36mLinearPlugin.run\u001b[0;34m(self, graph, config, updatehash)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(errors) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     77\u001b[0m     error, cause \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(errors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m raised. Re-raising first.\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     79\u001b[0m         error,\n\u001b[1;32m     80\u001b[0m     )\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcause\u001b[39;00m\n",
      "File \u001b[0;32m/home/liulab/anaconda3/envs/huangqi_fmriprep/lib/python3.8/site-packages/nipype/pipeline/plugins/linear.py:47\u001b[0m, in \u001b[0;36mLinearPlugin.run\u001b[0;34m(self, graph, config, updatehash)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_status_callback:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_status_callback(node, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m     \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdatehash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdatehash\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     49\u001b[0m     endstatus \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexception\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/home/liulab/anaconda3/envs/huangqi_fmriprep/lib/python3.8/site-packages/nipype/pipeline/engine/nodes.py:524\u001b[0m, in \u001b[0;36mNode.run\u001b[0;34m(self, updatehash)\u001b[0m\n\u001b[1;32m    521\u001b[0m savepkl(op\u001b[38;5;241m.\u001b[39mjoin(outdir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_inputs.pklz\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs\u001b[38;5;241m.\u001b[39mget_traitsfree())\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_interface\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    526\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[Node] Error on \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfullname, outdir)\n",
      "File \u001b[0;32m/home/liulab/anaconda3/envs/huangqi_fmriprep/lib/python3.8/site-packages/nipype/pipeline/engine/nodes.py:642\u001b[0m, in \u001b[0;36mNode._run_interface\u001b[0;34m(self, execute, updatehash)\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_hash()\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_results()\n\u001b[0;32m--> 642\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/liulab/anaconda3/envs/huangqi_fmriprep/lib/python3.8/site-packages/nipype/pipeline/engine/nodes.py:732\u001b[0m, in \u001b[0;36mNode._run_command\u001b[0;34m(self, execute, copyfiles)\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, MapNode):\n\u001b[1;32m    730\u001b[0m         dirs2keep \u001b[38;5;241m=\u001b[39m [op\u001b[38;5;241m.\u001b[39mjoin(outdir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmapflow\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m--> 732\u001b[0m     result\u001b[38;5;241m.\u001b[39moutputs \u001b[38;5;241m=\u001b[39m \u001b[43mclean_working_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneeded_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdirs2keep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirs2keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;66;03m# Store results file under all circumstances\u001b[39;00m\n\u001b[1;32m    742\u001b[0m _save_resultfile(\n\u001b[1;32m    743\u001b[0m     result,\n\u001b[1;32m    744\u001b[0m     outdir,\n\u001b[1;32m    745\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    746\u001b[0m     rebase\u001b[38;5;241m=\u001b[39mstr2bool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexecution\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_relative_paths\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m    747\u001b[0m )\n",
      "File \u001b[0;32m/home/liulab/anaconda3/envs/huangqi_fmriprep/lib/python3.8/site-packages/nipype/pipeline/engine/utils.py:1469\u001b[0m, in \u001b[0;36mclean_working_directory\u001b[0;34m(outputs, cwd, inputs, needed_outputs, config, files2keep, dirs2keep)\u001b[0m\n\u001b[1;32m   1467\u001b[0m outputdict \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mtrait_get()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs_to_keep:\n\u001b[0;32m-> 1469\u001b[0m     output_files\u001b[38;5;241m.\u001b[39mextend(walk_outputs(\u001b[43moutputdict\u001b[49m\u001b[43m[\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m]\u001b[49m))\n\u001b[1;32m   1470\u001b[0m needed_files \u001b[38;5;241m=\u001b[39m [path \u001b[38;5;28;01mfor\u001b[39;00m path, \u001b[38;5;28mtype\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output_files \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m str2bool(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexecution\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeep_inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n",
      "\u001b[0;31mKeyError\u001b[0m: 'subject_id'"
     ]
    }
   ],
   "source": [
    "# write the graph:\n",
    "wf.write_graph(graph2use='colored', simple_form=True)\n",
    "wf.run()\n",
    "# set the maximum resources the workflow can utilize:\n",
    "# args_dict = {'status_callback' : log_nodes_cb}\n",
    "# execute the workflow depending on the operating system:\n",
    "# if 'darwin' in sys.platform:\n",
    "#     # will execute the workflow using all available cpus:\n",
    "#     wf.run(plugin='MultiProc')\n",
    "# elif 'linux' in sys.platform:\n",
    "#     wf.run(plugin='PBS', plugin_args=dict(template=job_template))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (huangqi_fmriprep)",
   "language": "python",
   "name": "huangqi_fmriprep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
